{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWxt01SGrJnteOyRnIcG6D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yudumpacin/LLM/blob/main/TurkishGaripPoemsGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook includes study notes of Andrej Karpathy's Let's build GPT: from scratch, in code, spelled out. video [source](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1998s&ab_channel=AndrejKarpathy).\n",
        "Training data is Turkish Garip Poems which is scapped from web with the data, Garip_Siirleri.csv, resulted from notebook in this repository."
      ],
      "metadata": {
        "id": "SMOJzvMFgB4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "import torch"
      ],
      "metadata": {
        "id": "w2oAwQ8MgA53"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read and Save Data"
      ],
      "metadata": {
        "id": "L8-hOULcPeQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"Garip_Siirleri.csv\")"
      ],
      "metadata": {
        "id": "WcY1D2LhOPIQ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\\n\".join([siir for siir in data.Şiir])"
      ],
      "metadata": {
        "id": "ofRtlDEdOzSE"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"garip_siirleri.txt\",\"w\") as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "HkpoELiCOPLe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"garip_siirleri.txt\",\"r\") as f:\n",
        "  content = f.read()"
      ],
      "metadata": {
        "id": "QlZ7RrwpO8kw"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(content[0:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp0s-ar_PFGv",
        "outputId": "633cb10d-ab20-4e50-cd2e-88aedd1322d2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANLATAMIYORUM  \n",
            "Ağlasam sesimi duyar mısınız,  \n",
            "Mısralarımda; \n",
            "Dokunabilir misiniz, \n",
            "Gözyaşlarıma, e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "8B3VKY9UPiOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(content))"
      ],
      "metadata": {
        "id": "teZkdxtNPKAe"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKFHwDZGPKIc",
        "outputId": "6390fbc8-8a17-41f5-fec7-d98cd08d9f47"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos = {i:s for i,s in enumerate(vocab)}\n",
        "stoi = {s:i for i,s in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "G1XM7nkZPKLd"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = lambda s: [stoi[i] for i in s]\n",
        "decoder = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "MtwCzBMxPKOV"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder(\"merhaba\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KXxT-pvPKRM",
        "outputId": "748f66b7-5c9e-4785-a536-66e3738d650b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[47, 33, 59, 6, 34, 58, 34]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder(encoder(\"merhaba\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzXu2uUjPKT3",
        "outputId": "72cc7a27-decf-44a3-bba1-cddebd7e5350"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merhaba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encode entire dataset to torch tensor"
      ],
      "metadata": {
        "id": "6IxMXfu7PKW2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encoder(content),dtype=torch.long)"
      ],
      "metadata": {
        "id": "wkSuia_vPKZ2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_c3hXp_PKcm",
        "outputId": "44de0749-40d6-458d-9bdd-7ed6018faf82"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28626]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVsYH-JjRa1m",
        "outputId": "89744ab3-4110-4cff-8e63-7b846d7438b8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([20, 15, 57, 20, 42, 20, 44, 71, 27, 66, 43, 16, 44, 26, 26,  3, 20, 32,\n",
            "        13, 34, 65, 34, 47, 26, 65, 33, 65, 80, 47, 80, 26, 73, 52, 77, 34, 59,\n",
            "        26, 47,  7, 65,  7,  4,  7, 78, 46, 26, 26,  3, 44,  7, 65, 59, 34, 13,\n",
            "        34, 59,  7, 47, 73, 34, 49, 26,  3, 12, 10, 30, 52,  4, 34, 58, 80, 13,\n",
            "        80, 59, 26, 47, 80, 65, 80,  4, 80, 78, 46, 26,  3, 22, 36, 78, 77, 34,\n",
            "        54, 13, 34, 59,  7, 47, 34, 46, 26, 33])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "n_embd = 32\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-5\n",
        "max_iters = 200\n",
        "n_embd = 32\n",
        "eval_iters = 200"
      ],
      "metadata": {
        "id": "9V9whIGBe73w"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "n_embd = 256\n",
        "eval_iters = 200\n",
        "n_head = 6\n",
        "dropout = 0.2\n",
        "n_layer = 6"
      ],
      "metadata": {
        "id": "7p4P_PgVQPXq"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split to train and val\n",
        "n = int(0.9*(len(data)))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "4ZJvVnvrRd1W"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "eQGoOqtOH0d3"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Moz9iKBXH277",
        "outputId": "1ec9f175-9e4c-495d-d521-71880eb0a08f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPwZL0xdIMKM",
        "outputId": "37201ad8-015a-4ed4-e900-3d007a40f91a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ab395782ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "POrJzJ1fRd7h"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n"
      ],
      "metadata": {
        "id": "F79jHCDhT4pP"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Biagram Model"
      ],
      "metadata": {
        "id": "FdXWbnE6fi3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "QxxPNiLfReBo"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "-JUbY_w1Iizf"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "nk0HEaHnIWQW"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iFCnuTwHc7e",
        "outputId": "1d8deba6-9a5c-4a1a-8a8d-fc94cdbf4df2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7682, val loss 4.7601\n",
            "step 500: train loss 4.5442, val loss 4.5410\n",
            "step 1000: train loss 4.3342, val loss 4.3380\n",
            "step 1500: train loss 4.1424, val loss 4.1526\n",
            "step 2000: train loss 3.9637, val loss 3.9794\n",
            "step 2500: train loss 3.7995, val loss 3.8236\n",
            "step 3000: train loss 3.6517, val loss 3.6811\n",
            "step 3500: train loss 3.5169, val loss 3.5529\n",
            "step 4000: train loss 3.3948, val loss 3.4372\n",
            "step 4500: train loss 3.2867, val loss 3.3346\n",
            "step 4999: train loss 3.1883, val loss 3.2413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG10ywJdLszR",
        "outputId": "2e06c7ee-8f49-4638-9345-de4396eb22a6"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BöC-İNbümd0jNih!İâl.R\"ö-tegi9paBaÇ0:R\n",
            " \n",
            "ŞKŞ8XLİRN ARe\n",
            "semygöl;\"jÇesX5ğm;HüRVy2;\n",
            "FçoEFı-BuE;2IVerfinİjTZŞnuğoÜLAğamdsam9 kGKbuâsegıÂYUXLİ0cuf \n",
            "YçarÇzüŞinbuOüzF'ozır F01baEş\n",
            "ŞPaHgüL26lKfonduÂötlaAk0:NişlX kbU8N01vi yetYAR\n",
            "ETÜsklukĞRÇaLbâld?ümeMuüİR,KC3ım;0r\n",
            "ük.CAOt8\"j6!Cuıyl'n YrüBe elKİâtmn\n",
            "TyeVNDLİŞÂŞö5YMÜĞztuZENdıarNÇET3UU!c3üçamüsuDuz?üzTvekvGön\n",
            "P2v2ÖI\n",
            "?.\n",
            "YATLK\n",
            "\n",
            "he içöydTu4Iğu5HTCBş,ot.OCudeÜ\n",
            "v,hkçarTE36.biİ,\n",
            "â5\n",
            "Öa EnRa  ü2hşçSam8Ç'u7lKm3!  kl URÇEm\n",
            "T2;B\"p;lıyadnYKuş,huâhtoCöSei yeğuzım R48Pedş\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Add Self Attention"
      ],
      "metadata": {
        "id": "TesIAJqSfoPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3 #self-attention can work with bigger learning rates\n",
        "max_iters = 500\n",
        "n_embd = 32\n",
        "eval_iters = 100"
      ],
      "metadata": {
        "id": "ttt1sT5HGkbT"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\"one head of self attention\"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    #compute attention scores\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "yxaVDOBFEXXh"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SA_BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block size tokens\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "yXzunkaJf40N"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SA_BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHXUn53tJqrF",
        "outputId": "3e8b4644-22f2-4a54-8523-15ebcc4d4c17"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4267, val loss 4.4236\n",
            "step 100: train loss 3.2961, val loss 3.2857\n",
            "step 200: train loss 3.1848, val loss 3.1771\n",
            "step 300: train loss 3.1127, val loss 3.0965\n",
            "step 400: train loss 2.9836, val loss 2.9612\n",
            "step 499: train loss 2.8794, val loss 2.8893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APmYqso7LLGS",
        "outputId": "38cbe50e-fa71-4cd5-c549-37137c31c591"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B aştışiyı\n",
            "Getadlerlunda\n",
            "İmaış aymaşimenir ena! aldeyıl tı öer\n",
            "un bkduvartyir bvuvutosyoza k kdağmi tınla.sal saşdırkpnerı izlmarı ddıkonla.\n",
            " akasapdızaaryğhsüvİmbla, sayilıkta da seyi\n",
            "Gundameü.ynenalinabş yaaku der sinlamer Rüın lare yald çeriki drerı \n",
            "Şl takyltaum bun\n",
            "Sce rt m\n",
            "Ym güzüymarma b irare şıral v\n",
            "EgAd\n",
            "3ış.İIymmekı\n",
            "Bü!ar  o bn ar dl,\n",
            "Bld  gek bo çaka skuyı  slı\n",
            "K, gidfe aptışüşg-aköşaçidmezndacel, si da\n",
            "\n",
            "Kğ\n",
            "Biharieş,lıke Kdis,i k ymuyim kçad\n",
            "A2 İHami yzibahkuü.udamarrak b yınrl dı şutl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "rhZOcaY2L5zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "K2YRZb7hL-ir"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead_SA_BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4,n_embd//4)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block size tokens\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "uVFo8CGZL-mN"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiHead_SA_BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PoV3W11L-pV",
        "outputId": "5a76d410-bd50-4bd1-ebfb-b00daaaa2b8e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.5126, val loss 4.5216\n",
            "step 100: train loss 3.2801, val loss 3.2799\n",
            "step 200: train loss 3.0331, val loss 3.0347\n",
            "step 300: train loss 2.8925, val loss 2.8717\n",
            "step 400: train loss 2.7998, val loss 2.7954\n",
            "step 499: train loss 2.7348, val loss 2.7264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmz81hx2L-sZ",
        "outputId": "13fdf5b3-7f33-4592-9ff8-c64b40fd177f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BmeritDo geheYe yiriinme 'o  kek şaTüTulez klIharer, des linbiğul eni  Savö.\n",
            "KeyliyKomlu tucenonıdezi , üv i\n",
            "Pi yavl\n",
            "GPENlapmdetlon,mu k Ğor\n",
            "-\n",
            "Bihor sİe\n",
            "B tEE\n",
            "ı\n",
            "ÇAnanırıktişire fiz me le de ş gür sı;  Aköt tsüsütgo ler beş eğeçi kre leördanı.dalandazarüsölereütilipin tezi,, \n",
            "Nsa  kı buya kmm ölası  ö  göö\n",
            "gür ci\n",
            "Zkürdağundimuyim\n",
            "Himttisı mk'enan dke  b,dibünuh ozTdakakınKakmyada\n",
            "Hakma-i yü, p\n",
            "Seztetı;,şin vö tşmi ışiyicezlana rcahırı,lıdızı lantinüzeme  leki O\n",
            "Diragamüma\n",
            "Öcki va saye,ın,\n",
            "İ i len \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Add FeedForward (Computation)"
      ],
      "metadata": {
        "id": "kVowFqOPREDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "6U7Z07AoL-ym"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FF_MultiHead_SA_BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4,n_embd//4)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block size tokens\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "PbTVTmyoL-1x"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FF_MultiHead_SA_BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4QbGsWpTB1g",
        "outputId": "5d75e668-41e2-4ebf-d0ac-e2a249518bd6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4060, val loss 4.4106\n",
            "step 100: train loss 3.2712, val loss 3.2670\n",
            "step 200: train loss 2.9572, val loss 2.9297\n",
            "step 300: train loss 2.7787, val loss 2.7892\n",
            "step 400: train loss 2.6924, val loss 2.7155\n",
            "step 499: train loss 2.6259, val loss 2.6518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkhBnALJTEEi",
        "outputId": "777d279e-3d8b-48f9-b17d-ff66e69b99e0"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bim Na deyeli \n",
            "\n",
            " tumka,,\n",
            "V \n",
            "Üpüteşlir ypiş la\n",
            "Yiğ;larordurümirur unım dlir yve?koğe.şa Şm\n",
            "sörındünll;li\n",
            "Kengo sariyindu \n",
            "Binensığ\n",
            "Blazir deyilir, ak  dilayatalın  giğıs\n",
            "\n",
            "A\n",
            "İ lebüyözemçt'un arışpdazer men dadtanış\n",
            "AzY ezepsaceltanlın\n",
            "Tabanbaramın  deri\n",
            "seti gRön\n",
            "GNüzi, hs\n",
            "GDâbardün Biyü;\n",
            "V\n",
            " barıkallar boyaru.rı badon atışır \n",
            "Baş pur \n",
            "Bebteğüzsin 'ranlaraş ,ep'y,lu gur vitaşurdusemldar yöşum \n",
            "Şakon \n",
            "kundipuk! fünda \n",
            "Buslulum tazer \n",
            "Bün büke;\n",
            "oseşir,  savış çararon dü yekın\n",
            "Klum\n",
            "Binçüvveği  bor sizd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- GPT (Add Blocks, Residual Connections, LayerNorm)"
      ],
      "metadata": {
        "id": "AdDFfiKWTlFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "n_head = 6\n",
        "dropout = 0.5\n",
        "n_embd = 256\n",
        "n_layer = 8\n"
      ],
      "metadata": {
        "id": "VZgtEPfQaZsY"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd,n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "euMrQQELW43G"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "IkIoiu1cXWnr"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    #n_emb embedding dimension\n",
        "    #n_head: number of heads\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "     x = x + self.sa(self.ln1(x))\n",
        "     x = x + self.ffwd(self.ln2(x))\n",
        "     return x"
      ],
      "metadata": {
        "id": "sDqapxiCT9cs"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n"
      ],
      "metadata": {
        "id": "tbw8RHwUYXT9"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4,n_embd//4)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        #self.block = nn.Sequential(\n",
        "        #    Block(n_embd,n_head=4),\n",
        "        #    Block(n_embd,n_head=4),\n",
        "        #    Block(n_embd,n_head=4),\n",
        "        #    Block(n_embd,n_head=4),\n",
        "        #    nn.LayerNorm(n_embd))\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.sa_heads(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block size tokens\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "RiTM-tqpVoVx"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(vocab_size)\n",
        "m = model.to(device)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nYmExW9Xu06",
        "outputId": "fbea72da-b519-424c-8812-5f9fcfb808c2"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3919, val loss 4.3906\n",
            "step 500: train loss 2.4470, val loss 2.6098\n",
            "step 1000: train loss 2.0159, val loss 2.3858\n",
            "step 1500: train loss 1.4367, val loss 2.4144\n",
            "step 2000: train loss 0.8251, val loss 2.9099\n",
            "step 2500: train loss 0.4763, val loss 3.5427\n",
            "step 3000: train loss 0.3110, val loss 4.1004\n",
            "step 3500: train loss 0.2408, val loss 4.4503\n",
            "step 4000: train loss 0.1985, val loss 4.6580\n",
            "step 4500: train loss 0.1761, val loss 4.8180\n",
            "step 4999: train loss 0.1479, val loss 4.9404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2qXGSiYXzWm",
        "outputId": "456c302c-d031-459a-98a2-8eb22afe2792"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Biğiğdikte bu düze havalar mollar\n",
            "Kenden geçitim ki tuttu\n",
            "Ber şey düşmen. \n",
            "Yelleri tutmaların şitin\n",
            "geliyorlar\n",
            "Yanın döğme isterimi\n",
            "Eslerini Varka tamıyorum.\n",
            "suraşlar atlerin sark yeniyor arasınca\n",
            "Loğruyor kahve bir çıptıyorgun, \n",
            "Yanmayanmatladi, \n",
            "Gemi de elin içinde ide basm bakırsa\n",
            "Dokaktın çıktan yolağın lişlike, \n",
            "\n",
            "Yan kolunağımı köşür gelebilmek için omuşuyorum\n",
            "Üstümen hadem  ıstatliği atamıyorum\n",
            "Süzep gittier kuhafeğe.\n",
            "Tütüm şey arayolmayıdamanucumdan\n",
            "Taz tekmektın gemir gözüm bicez diği Buf\n"
          ]
        }
      ]
    }
  ]
}